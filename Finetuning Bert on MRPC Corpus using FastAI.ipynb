{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook requires Python >= 3.6 and fastai==1.0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "In this notebook we will finetune pre-trained BERT model on The Microsoft Research Paraphrase Corpus (MRPC). MRPC is a paraphrase identification dataset, where systems aim to identify if two sentences are paraphrases of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "### Overview: \n",
    "1. Trained on BookCorpus and English Wikipedia (800M and 2,500M words respectively).\n",
    "2. Training time approx. about a week using 64 GPUs.\n",
    "3. State-Of-The-Art (SOTA) results on SQuAD v1.1 and all 9 GLUE benchmark tasks.\n",
    "\n",
    "### Core Components:\n",
    "1. Masked LM (MLM): Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token.\n",
    "2. Next Sentence Prediction (NSP): In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. \n",
    "3. BERT uses wordpieces (e.g. playing -> play + ##ing) instead of words reducing the size of the vocabulary and increases the amount of data that is available for each word.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "<img src=\"images/input.png\">\n",
    "\n",
    "- Token embeddings, Segment embeddings and Position embeddings. \n",
    "- [SEP] token to mark the end of a sentence.\n",
    "- [CLS] token at the beginning of the input sequence, to be used only if classifying.\n",
    "- BERT BASE (12 encoders) and BERT LARGE (24 encoders)\n",
    "\n",
    "- The convention in BERT is:\n",
    "\n",
    "(a) For sequence pairs:\n",
    " tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    " \n",
    " type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
    " \n",
    "(b) For single sequences:\n",
    " tokens:   [CLS] the dog is hairy . [SEP]\n",
    " \n",
    " type_ids:   0   0   0   0  0     0   0\n",
    "\n",
    "Where \"type_ids\" are used to indicate whether this is the first\n",
    "sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "`type=1` were learned during pre-training and are added to the wordpiece\n",
    "embedding vector (and position vector). This is not *strictly* necessary\n",
    "since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "it easier for the model to learn the concept of sequences.\n",
    "        \n",
    "- For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "used as as the \"sentence vector\". The  first  token  of  every  sequence  is  always  the  special  classification  embedding([CLS]). The  final  hidden  state  (i.e.,  out-put of Transformer) corresponding to this token  is  used  as  the  aggregate  sequence  rep-resentation for classification tasks. For non-classification tasks, this vector is ignored.\n",
    "\n",
    "**Note that this only makes sense because the entire model is fine-tuned.** \n",
    "\n",
    "BERT is bidirectional, the [CLS] is encoded including all representative information of all tokens through the multi-layer encoding procedure. The representation of [CLS] is individual in different sentences. [CLS] is later fine-tuned on the downstream task. Only after fine-tuning, [CLS] aka the first token can be a meaningful representation of the whole sentence. [Link](https://github.com/google-research/bert/issues/196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Model\n",
    "\n",
    "We will use op-for-op PyTorch reimplementation of Google's BERT model provided by pytorch-pretrained-BERT library. Refer the Github repo for more info: https://github.com/huggingface/pytorch-pretrained-BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This repo contains useful classes to use BERT with FastAI\n",
    "\n",
    "#! git clone https://github.com/deepklarity/fastai-bert-finetuning.git fastai_bert_finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.cm as cm\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.metrics import *\n",
    "import utils, bert_fastai, bert_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed random generators so all model runs are reproducible\n",
    "\n",
    "utils.seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BERT with fastai\n",
    "\n",
    "There are three things we need to be careful of when using BERT with fastai:\n",
    "\n",
    "   1. BERT uses its own wordpiece tokenizer.\n",
    "    \n",
    "   [WordPiece](https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944) is a commonly used technique to segment words into subword-level in NLP tasks.\n",
    "       The vocabulary is initialized with all the individual characters in the language, and\n",
    "       then the most frequent/likely combinations of the symbols in the vocabulary are iteratively\n",
    "       added to the vocabulary.\n",
    "       \n",
    "   **How does this help?**\n",
    "\n",
    "  Imagine that the model sees the word walking. Unless this word occurs at least a few times in the\n",
    "training corpus, the model can't learn to deal with this word very well. However, it may have the\n",
    "words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these\n",
    "words are treated as completely different words by the model.\n",
    "However, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have\n",
    "walk@@ in common, which will occur much frequently while training, and the model might be able to\n",
    "learn more about it.\n",
    "   \n",
    "   2. BERT needs [CLS] and [SEP] tokens added to each sequence.\n",
    "    \n",
    "  For classification tasks we need to add [CLS] token before every input sequence.\n",
    "[SEP] token is used to separate sentences.\n",
    "    \n",
    "   3. BERT uses its own pre-built vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Language Model Demo\n",
    "\n",
    "We will load a BERT model with the masked language modeling head and predict masked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_token_model = bert_helper.BertMaskedLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Steve Jobs founded apple .  [SEP]\n",
      "[CLS] Microsoft makes windows .  [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = '[CLS] Steve Jobs founded [MASK] . [SEP][CLS] Microsoft makes [MASK] . [SEP]'\n",
    "preds = bert_token_model.predict_tokens(text)\n",
    "for p in preds: print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_token_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "MRPC dataset a text file containing 5800 pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The column named **Quality** indicates whether the sentences are similar (1) or not (0). **\"#1 String\" and \"#2 String\"** columns contain the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.microsoft.com/en-us/download/details.aspx?id=52398\n",
    "# Microsoft Research Paraphrase Corpus\n",
    "TASK='MRPC'\n",
    "DATA_ROOT = Path(\".\")\n",
    "label_col = \"Quality\"\n",
    "text_cols = [\"#1 String\", \"#2 String\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing MRPC...\n",
      "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "# Execute script to download MRPC data and create train.csv and test.csv\n",
    "\n",
    "! python download_glue_data.py --data_dir='glue_data' --tasks=$TASK --test_labels=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>702876</td>\n",
       "      <td>702977</td>\n",
       "      <td>Amrozi accused his brother , whom he called \" ...</td>\n",
       "      <td>Referring to him as only \" the witness \" , Amr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2108705</td>\n",
       "      <td>2108831</td>\n",
       "      <td>Yucaipa owned Dominick 's before selling the c...</td>\n",
       "      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1330381</td>\n",
       "      <td>1330521</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10 , the ship 's owners had published ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3344667</td>\n",
       "      <td>3344648</td>\n",
       "      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n",
       "      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1236820</td>\n",
       "      <td>1236712</td>\n",
       "      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n",
       "      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quality    #1 ID    #2 ID  \\\n",
       "0        1   702876   702977   \n",
       "1        0  2108705  2108831   \n",
       "2        1  1330381  1330521   \n",
       "3        0  3344667  3344648   \n",
       "4        1  1236820  1236712   \n",
       "\n",
       "                                           #1 String  \\\n",
       "0  Amrozi accused his brother , whom he called \" ...   \n",
       "1  Yucaipa owned Dominick 's before selling the c...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT , Tab shares were up 19 cents ...   \n",
       "4  The stock rose $ 2.11 , or about 11 percent , ...   \n",
       "\n",
       "                                           #2 String  \n",
       "0  Referring to him as only \" the witness \" , Amr...  \n",
       "1  Yucaipa bought Dominick 's in 1995 for $ 693 m...  \n",
       "2  On June 10 , the ship 's owners had published ...  \n",
       "3  Tab shares jumped 20 cents , or 4.6 % , to set...  \n",
       "4  PG & E Corp. shares jumped $ 1.63 or 8 percent...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(DATA_ROOT / \"glue_data\" / \"MRPC\" / \"train.tsv\", sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1089874</td>\n",
       "      <td>1089925</td>\n",
       "      <td>PCCW 's chief operating officer , Mike Butcher...</td>\n",
       "      <td>Current Chief Operating Officer Mike Butcher a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3019446</td>\n",
       "      <td>3019327</td>\n",
       "      <td>The world 's two largest automakers said their...</td>\n",
       "      <td>Domestic sales at both GM and No. 2 Ford Motor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1945605</td>\n",
       "      <td>1945824</td>\n",
       "      <td>According to the federal Centers for Disease C...</td>\n",
       "      <td>The Centers for Disease Control and Prevention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1430402</td>\n",
       "      <td>1430329</td>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "      <td>A tropical storm rapidly developed in the Gulf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3354381</td>\n",
       "      <td>3354396</td>\n",
       "      <td>The company didn 't detail the costs of the re...</td>\n",
       "      <td>But company officials expect the costs of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Quality    #1 ID    #2 ID  \\\n",
       "0      0        1  1089874  1089925   \n",
       "1      1        1  3019446  3019327   \n",
       "2      2        1  1945605  1945824   \n",
       "3      3        0  1430402  1430329   \n",
       "4      4        0  3354381  3354396   \n",
       "\n",
       "                                           #1 String  \\\n",
       "0  PCCW 's chief operating officer , Mike Butcher...   \n",
       "1  The world 's two largest automakers said their...   \n",
       "2  According to the federal Centers for Disease C...   \n",
       "3  A tropical storm rapidly developed in the Gulf...   \n",
       "4  The company didn 't detail the costs of the re...   \n",
       "\n",
       "                                           #2 String  \n",
       "0  Current Chief Operating Officer Mike Butcher a...  \n",
       "1  Domestic sales at both GM and No. 2 Ford Motor...  \n",
       "2  The Centers for Disease Control and Prevention...  \n",
       "3  A tropical storm rapidly developed in the Gulf...  \n",
       "4  But company officials expect the costs of the ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_ROOT / \"glue_data\" / \"MRPC\" / \"test.tsv\", sep = '\\t', quoting=csv.QUOTE_NONE)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training records=3668\n",
      "Number of Test records=1725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Training records={len(train_df)}\")\n",
    "print(f\"Number of Test records={len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_sentences(quality, n=5):\n",
    "    ctr = 0\n",
    "    for row in train_df.query(f'Quality=={quality}').itertuples():\n",
    "        print(f\"1. {row[4]}\\n2. {row[5]}\")\n",
    "        print(\"=\"*100)\n",
    "        ctr += 1\n",
    "        if n==ctr:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\n",
      "2. Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
      "====================================================================================================\n",
      "1. Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .\n",
      "2. Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .\n",
      "====================================================================================================\n",
      "1. The Nasdaq had a weekly gain of 17.27 , or 1.2 percent , closing at 1,520.15 on Friday .\n",
      "2. The tech-laced Nasdaq Composite .IXIC rallied 30.46 points , or 2.04 percent , to 1,520.15 .\n",
      "====================================================================================================\n",
      "1. That compared with $ 35.18 million , or 24 cents per share , in the year-ago period .\n",
      "2. Earnings were affected by a non-recurring $ 8 million tax benefit in the year-ago period .\n",
      "====================================================================================================\n",
      "1. Shares of Genentech , a much larger company with several products on the market , rose more than 2 percent .\n",
      "2. Shares of Xoma fell 16 percent in early trade , while shares of Genentech , a much larger company with several products on the market , were up 2 percent .\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Different sentences samples            \n",
    "sample_sentences(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "2. Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "====================================================================================================\n",
      "1. They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .\n",
      "2. On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\n",
      "====================================================================================================\n",
      "1. The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .\n",
      "2. PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\n",
      "====================================================================================================\n",
      "1. Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier .\n",
      "2. With the scandal hanging over Stewart 's company , revenue the first quarter of the year dropped 15 percent from the same period a year earlier .\n",
      "====================================================================================================\n",
      "1. The DVD-CCA then appealed to the state Supreme Court .\n",
      "2. The DVD CCA appealed that decision to the U.S. Supreme Court .\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Similar sentences samples            \n",
    "sample_sentences(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Specify BERT configs\n",
    "\n",
    "config = utils.Config(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    num_labels=2, # 0 or 1\n",
    "    max_lr=2e-5,\n",
    "    epochs=4,\n",
    "    batch_size=32,\n",
    "    max_seq_len=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = bert_fastai.FastAITokenizer(model_name=config.bert_model_name, max_seq_len=config.max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can build the databunch using the tokenizer we build above. Notice we're passing the `include_bos=False` and `include_eos=False` options. This is to prevent fastai from adding its own Start-Of-Sentence (SOS)/End-Of-Sentence (EOS) tokens that will interfere with BERT's SOS/EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = TextDataBunch.from_df(\".\", train_df=train_df, valid_df=test_df,\n",
    "                  tokenizer=fastai_tokenizer.bert_tokenizer(),\n",
    "                  vocab=fastai_tokenizer.fastai_bert_vocab(),\n",
    "                  include_bos=False,\n",
    "                  include_eos=False,\n",
    "                  text_cols=text_cols,\n",
    "                  label_cols=label_col,\n",
    "                  bs=config.batch_size,\n",
    "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original==> Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .,Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "\n",
      "Tokenized==>. [CLS] am ##ro ##zi accused his brother , whom he called \" the witness \" , of deliberately di ##stor ##ting his evidence . referring to him as only \" the witness \" , am ##ro ##zi accused his brother of deliberately di ##stor ##ting his evidence . [SEP]\n",
      "====================================================================================================\n",
      "Original==> Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .,Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\n",
      "\n",
      "Tokenized==>. [CLS] yu ##ca ##ip ##a owned dominic ##k ' s before selling the chain to safe ##way in 1998 for $ 2 . 5 billion . yu ##ca ##ip ##a bought dominic ##k ' s in 1995 for $ 69 ##3 million and sold it to safe ##way for $ 1 . 8 billion in 1998 . [SEP]\n",
      "====================================================================================================\n",
      "Original==> They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .,On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\n",
      "\n",
      "Tokenized==>. [CLS] they had published an advertisement on the internet on june 10 , offering the cargo for sale , he added . on june 10 , the ship ' s owners had published an advertisement on the internet , offering the explosives for sale . [SEP]\n",
      "====================================================================================================\n",
      "Original==> Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .,Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .\n",
      "\n",
      "Tokenized==>. [CLS] around 03 ##35 gm ##t , tab shares were up 19 cents , or 4 . 4 % , at a $ 4 . 56 , having earlier set a record high of a $ 4 . 57 . tab shares jumped 20 cents , or 4 . 6 % , to set a record closing high at a $ 4 . 57 . [SEP]\n",
      "====================================================================================================\n",
      "Original==> The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .,PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\n",
      "\n",
      "Tokenized==>. [CLS] the stock rose $ 2 . 11 , or about 11 percent , to close friday at $ 21 . 51 on the new york stock exchange . pg & e corp . shares jumped $ 1 . 63 or 8 percent to $ 21 . 03 on the new york stock exchange on friday . [SEP]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show wordpiece tokenized data\n",
    "\n",
    "for i in range(5): \n",
    "    print(f\"Original==> {train_df.loc[i][text_cols[0]]},{train_df.loc[i][text_cols[1]]}\\n\\nTokenized==>. {databunch.x[i]}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the data in place, we will prepare the model. Again, the pytorch-pretrained-bert package gives us a sequence classifier based on BERT straight out of the box. We also build FastAI `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\n",
    "\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    config.bert_model_name, num_labels=config.num_labels)\n",
    "\n",
    "learner = bert_fastai.BertLearner(databunch,\n",
    "                                  bert_model,\n",
    "                                  metrics=[accuracy])\n",
    "\n",
    "learner.callbacks.append(ShowGraph(learner))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the accuracy and f1_score of the pre-trained model on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.5281159281730652, f1_score=0.6238447319778189\n"
     ]
    }
   ],
   "source": [
    "preds, pred_values, true_labels = learner.get_predictions()\n",
    "learner.print_metrics(preds, pred_values, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_ci = TextClassificationInterpretation.from_learner(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Index</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Evidence suggests two of the victims were taken by surprise , while the other two might have tried to flee or to defend themselves or the others , police said .,Evidence suggests two victims were taken by surprise , while the others may have tried to flee or perhaps defend themselves or their friends , police said .</td>\n",
       "      <td>[CLS] evidence suggests two of the victims were taken by surprise , while the other two might have tried to flee or to defend themselves or the others , police said . evidence suggests two victims were taken by surprise , while the others may have tried to flee or perhaps defend themselves or their friends , police said . [SEP]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>In an E-mail statement to the Knoxville News Sentinel , Shumaker said , ' ' I am not giving any consideration to resignation .,I am not giving any consideration to resignation , \" Shumaker said in a statement .</td>\n",
       "      <td>[CLS] in an e - mail statement to the knoxville news sentinel , shu ##maker said , ' ' i am not giving any consideration to resignation . i am not giving any consideration to resignation , \" shu ##maker said in a statement . [SEP]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>\" I expect Japan to keep conducting intervention , but the volume is likely to fall sharply , \" said Junya Tanase , forex strategist at JP Morgan Chase .,Junya Tanase , forex strategist at JP Morgan Chase , said \" I expect Japan to keep conducting intervention , but the volume is likely to fall sharply . \"</td>\n",
       "      <td>[CLS] \" i expect japan to keep conducting intervention , but the volume is likely to fall sharply , \" said jun ##ya tan ##ase , fore ##x st ##rate ##gist at jp morgan chase . jun ##ya tan ##ase , fore ##x st ##rate ##gist at jp morgan chase , said \" i expect japan to keep conducting intervention , but the volume is likely to fall sharply .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>The Episcopal Church ' ' is alienating itself from the Anglican Communion , ' ' said the Very Rev. Peter Karanja , provost of the All Saints Cathedral , in Nairobi .,In Nairobi , the provost of All Saints Cathedral , the Very Reverend Peter Karanja , said the US Episcopal Church was alienating itself from the Anglican Communion .</td>\n",
       "      <td>[CLS] the episcopal church ' ' is alien ##ating itself from the anglican communion , ' ' said the very rev . peter kara ##n ##ja , provost of the all saints cathedral , in nairobi . in nairobi , the provost of all saints cathedral , the very reverend peter kara ##n ##ja , said the us episcopal church was alien ##ating itself from the anglican communion . [SEP]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>Three Southern politicians who ``stood up to ancient hatreds ' ' were honored Monday with Profile in Courage Awards from the John F. Kennedy Library and Museum .,Barnes is one of three politicians honored Monday by the John F. Kennedy Library and Museum with the Profile in Courage Award .</td>\n",
       "      <td>[CLS] three southern politicians who ` ` stood up to ancient hatred ##s ' ' were honored monday with profile in courage awards from the john f . kennedy library and museum . barnes is one of three politicians honored monday by the john f . kennedy library and museum with the profile in courage award . [SEP]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.custom_show_top_losses(txt_ci, test_df, text_cols, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.571216</td>\n",
       "      <td>0.704928</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472451</td>\n",
       "      <td>0.414120</td>\n",
       "      <td>0.818551</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.313452</td>\n",
       "      <td>0.416431</td>\n",
       "      <td>0.819130</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230470</td>\n",
       "      <td>0.432191</td>\n",
       "      <td>0.834783</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(config.epochs, max_lr=config.max_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.834782600402832, f1_score=0.8806032677000418\n"
     ]
    }
   ],
   "source": [
    "preds, pred_values, true_labels = learner.get_predictions()\n",
    "learner.print_metrics(preds, pred_values, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_ci = TextClassificationInterpretation.from_learner(learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Index</th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>Chante Jawan Mallard , 27 , went on trial Monday , charged with first-degree murder .,Chante Jawaon Mallard , 27 , is charged with murder and tampering with evidence .</td>\n",
       "      <td>[CLS] chant ##e jaw ##an mall ##ard , 27 , went on trial monday , charged with first - degree murder . chant ##e jaw ##ao ##n mall ##ard , 27 , is charged with murder and tam ##per ##ing with evidence . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>His chief lawyer , Mahendradatta , said Bashir was mentally prepared for a heavy sentencing demand and felt the Marriott bombing would affect the decision .,A lawyer for Bashir , Mahendradatta , said earlier his client was mentally prepared for a heavy sentencing demand and had felt the Marriott bombing would affect the decision .</td>\n",
       "      <td>[CLS] his chief lawyer , ma ##hend ##rada ##tta , said bash ##ir was mentally prepared for a heavy sentencing demand and felt the marriott bombing would affect the decision . a lawyer for bash ##ir , ma ##hend ##rada ##tta , said earlier his client was mentally prepared for a heavy sentencing demand and had felt the marriott bombing would affect the decision . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.12</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>GE stock were up 37 cents to $ 28.56 in morning New York Stock Exchange trade .,Investors reacted little , with GE shares edging 7 cents lower to end at $ 28.12 on the New York Stock Exchange .</td>\n",
       "      <td>[CLS] ge stock were up 37 cents to $ 28 . 56 in morning new york stock exchange trade . investors reacted little , with ge shares ed ##ging 7 cents lower to end at $ 28 . 12 on the new york stock exchange . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>Casteen has been under pressure from Gov. Mark R. Warner and other state officials to do whatever he could to protect Virginia Tech 's athletic viability .,Virginia Gov. Mark R. Warner has been urging other state officials to do whatever they could to protect Virginia Tech 's interests .</td>\n",
       "      <td>[CLS] caste ##en has been under pressure from gov . mark r . warner and other state officials to do whatever he could to protect virginia tech ' s athletic via ##bility . virginia gov . mark r . warner has been urging other state officials to do whatever they could to protect virginia tech ' s interests . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.09</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>\" The Domino application server is going to be around for at least the next decade . \",\" The Domino application server will be around for the next decade , \" he said .</td>\n",
       "      <td>[CLS] \" the domino application server is going to be around for at least the next decade . \" \" the domino application server will be around for the next decade , \" he said . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.07</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>\" I felt that if I disagreed with Rosie too much I would lose my job , \" she said .,Cavender did say : \" I felt that if I disagreed with Rosie too much I would lose my job . \"</td>\n",
       "      <td>[CLS] \" i felt that if i disagreed with rosie too much i would lose my job , \" she said . cave ##nder did say : \" i felt that if i disagreed with rosie too much i would lose my job . \" [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>Misfeldt agreed to pay $ US346,807 in allegedly ill-gotten gains and $ US111,595 in interest .,Mr. Misfeldt agreed to surrender $ 346,807 in trading profits and $ 111,595 in interest .</td>\n",
       "      <td>[CLS] mis ##feld ##t agreed to pay $ us ##34 ##6 , 80 ##7 in allegedly ill - gotten gains and $ us ##11 ##1 , 59 ##5 in interest . mr . mis ##feld ##t agreed to surrender $ 34 ##6 , 80 ##7 in trading profits and $ 111 , 59 ##5 in interest . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>\" I have lots of bad dreams , I have flashbacks , I have lots of anger .,\" I have lots of bad dreams , flashbacks and lots of anger . \"</td>\n",
       "      <td>[CLS] \" i have lots of bad dreams , i have flashbacks , i have lots of anger . \" i have lots of bad dreams , flashbacks and lots of anger . \" [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>Continuing the record-setting pace of recent years , personal bankruptcies rose 7.8 percent in the 12 months ending Sept . 30 , the Administrative Office of the U.S. Courts said Friday .,The record-setting pace of new personal bankruptcies continued in the 12 months ending Sept . 30 , with their number rising 7.8 percent , according to data released Friday .</td>\n",
       "      <td>[CLS] continuing the record - setting pace of recent years , personal bankrupt ##cies rose 7 . 8 percent in the 12 months ending sept . 30 , the administrative office of the u . s . courts said friday . the record - setting pace of new personal bankrupt ##cies continued in the 12 months ending sept . 30 , with their number rising 7 . 8 percent ,</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>They found that 18 percent of the men who took finasteride , or 803 men , developed prostate cancer .,About 24 percent of men who took the placebo , or 1,147 men , developed prostate cancer .</td>\n",
       "      <td>[CLS] they found that 18 percent of the men who took fin ##aster ##ide , or 80 ##3 men , developed prostate cancer . about 24 percent of men who took the place ##bo , or 1 , 147 men , developed prostate cancer . [SEP]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.custom_show_top_losses(txt_ci, test_df, text_cols, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "\n",
    "- There's major improvement in the accuracy and loss by just running for few epochs. We can improve the model further to acc = 0.8504901960784313 and f1 = 0.8974789915966387 by using the techniques mentioned in the original BERT paper. Refer to https://github.com/huggingface/pytorch-pretrained-BERT#mrpc for details.\n",
    "- Use BertAdam Optimizer [BertAdam](https://github.com/huggingface/pytorch-pretrained-BERT/blob/694e2117f33d752ae89542e70b84533c52cb9142/README.md#optimizers)\n",
    "[Is BertAdam better](https://github.com/huggingface/pytorch-pretrained-BERT/issues/420)\n",
    "\n",
    "- Finetune BERT on Quora insincere questions.\n",
    "- Explore LAMB Optimizer: https://forums.fast.ai/t/lamb-optimizer/43582/19\n",
    "- Ulmfit , gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
